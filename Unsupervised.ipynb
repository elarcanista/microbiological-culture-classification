{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminars"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import random\n",
    "import colorsys\n",
    "from functools import partial\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "filename = 'Iris.csv'\n",
    "column_category = 'Species'\n",
    "columns_plot = ('SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm')\n",
    "sample_proportions = {'Train': 0.6, 'Validation': 0.2, 'Test': 0.2}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_clusters(clusters, attributes):\r\n",
    "    fig, axs = plt.subplots(1, len(clusters)+1, subplot_kw=dict(projection='3d'))\r\n",
    "\r\n",
    "    colors = [colorsys.hsv_to_rgb(i/len(clusters), 1, 1) for i in range(len(clusters))]\r\n",
    "\r\n",
    "    for ax, (name, data), color in zip(axs, clusters.items(), colors):\r\n",
    "        ax.set_title(name)\r\n",
    "        ax.scatter(data.loc[:, attributes[0]],\r\n",
    "                data.loc[:, attributes[1]],\r\n",
    "                data.loc[:, attributes[2]], color=color)\r\n",
    "        ax.set_xlabel(attributes[0])\r\n",
    "        ax.set_ylabel(attributes[1])\r\n",
    "        ax.set_zlabel(attributes[2])\r\n",
    "        axs[-1].scatter(data.loc[:, attributes[0]],\r\n",
    "                data.loc[:, attributes[1]],\r\n",
    "                data.loc[:, attributes[2]], color=color)\r\n",
    "\r\n",
    "    axs[-1].set_title('Total')\r\n",
    "    axs[-1].set_xlabel(attributes[0])\r\n",
    "    axs[-1].set_ylabel(attributes[1])\r\n",
    "    axs[-1].set_zlabel(attributes[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sampling_uniform(data, sample_proportions):\n",
    "    assert sum(sample_proportions.values()) == 1, sum(sample_proportions.values())\n",
    "\n",
    "    shuffled_data = data.sample(frac=1)\n",
    "    slice_low = 0\n",
    "    samples = {}\n",
    "    for sample, proportion in sample_proportions.items():\n",
    "        slice_high = slice_low + proportion\n",
    "        samples[sample] = shuffled_data.iloc[int(len(shuffled_data) * slice_low) : int(len(shuffled_data) * slice_high), :]\n",
    "        slice_low = slice_high\n",
    "\n",
    "    return samples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distances"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dist_euclidean2(p1, p2):\n",
    "    d = p1 - p2\n",
    "    return d.dot(d)\n",
    "\n",
    "def dist_manhattan(p1, p2):\n",
    "    return abs(p1 - p2).sum()\n",
    "\n",
    "# precision matrix is the inverse of the covariance matrix\n",
    "def dist_mahalanobis2(precision, p1, p2):\n",
    "    d = p1 - p2\n",
    "    return d.transpose().dot(precision).dot(d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def centroids(points, k):\n",
    "    return [points[points['Cluster'] == cluster].mean() for cluster in range(k)]\n",
    "    \n",
    "def init_randompartition(points, k):\n",
    "    points['Cluster'] = [random.randrange(0, k) for i in range(points.shape[0])]\n",
    "    means = centroids(points, k)\n",
    "    return points, means\n",
    "\n",
    "def cluster_kmeans(points, dist, k):\n",
    "    points, means = init_randompartition(points, k)\n",
    "\n",
    "    while True:\n",
    "        for r in points.iterrows():\n",
    "            print(r)\n",
    "            print(list(map(partial(dist, points.iloc[1]), means)))\n",
    "        print(points.iloc[1], np.argmin(map(partial(dist, points.iloc[1]), means)))\n",
    "        clusters_new = [np.argmin(map(partial(dist, r), means)) for r in points.iterrows()]\n",
    "        points['Cluster'] = clusters_new\n",
    "        means_new = centroids(points, k)\n",
    "        delta = np.mean([dist(p1, p2) for p1, p2 in zip(means, means_new)])\n",
    "        assert not np.isnan(delta)\n",
    "        print(delta)\n",
    "        if(delta < 0.01):\n",
    "            return points, means_new\n",
    "        means = means_new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(filename, index_col=0)\n",
    "sample = sampling_uniform(data.drop([column_category], axis=1), sample_proportions)\n",
    "plot_clusters(sample, columns_plot)\n",
    "cluster_kmeans(sample['Train'], dist_euclidean2, 3)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "1a74cfba15cb2a7e1711077e0bf40c4aa90b3e0e84105adac214ba76b6a8694a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}