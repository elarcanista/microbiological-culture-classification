{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminars"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import random\n",
    "import colorsys\n",
    "from functools import partial\n",
    "from functools import reduce\n",
    "\n",
    "filename = 'Iris.csv'\n",
    "sample_proportions = {'Train': 0.6, 'Validation': 0.2, 'Test': 0.2}\n",
    "k = 3\n",
    "\n",
    "data = pd.read_csv(filename, index_col=0)\n",
    "name_to_index = {name: i for i, name in enumerate(data.iloc[:, -1].unique())}\n",
    "i_to_category = {i: name for name, i in name_to_index.items()}\n",
    "data.iloc[:, -1] = data.iloc[:, -1].apply(lambda x: name_to_index[x])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_clusters(data, i_to_name, centroids=pd.DataFrame(columns=data.columns)):\n",
    "    def get_color(N, n): return colorsys.hsv_to_rgb(n/N, 0.65, 1)\n",
    "\n",
    "    def get_colorN(x): return get_color(len(i_to_name), x)\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "    fig, axs = plt.subplots(data.shape[1]-1, data.shape[1]-1,\n",
    "                            sharex='col', sharey='row', constrained_layout=True)\n",
    "\n",
    "    colors = data.iloc[:, -1].apply(get_colorN)\n",
    "    color_clusters = [get_colorN(i) for i in range(len(i_to_name))]\n",
    "    color_centroids = [get_colorN(i) for i in range(centroids.shape[0])]\n",
    "\n",
    "    for r, r_name in enumerate(data.columns[:-1]):\n",
    "        y_data = data.iloc[:, r]\n",
    "        y_centroids = centroids.iloc[:, r]\n",
    "        for c, c_name in enumerate(data.columns[:-1]):\n",
    "            if r != c:\n",
    "                axs[r, c].scatter(data.iloc[:, c], y_data, c=colors)\n",
    "                axs[r, c].scatter(centroids.iloc[:, c], y_centroids,\n",
    "                                  c=color_centroids, marker=(5, 1), edgecolors='black')\n",
    "            else:\n",
    "                axs[r, c].text(0.5, 0.5, r_name,\n",
    "                               fontweight='bold',\n",
    "                               horizontalalignment='center',\n",
    "                               verticalalignment='center',\n",
    "                               transform=axs[r, c].transAxes)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], label=i_to_name[i], markerfacecolor=c,\n",
    "                              color='w', marker='s', markersize=12)\n",
    "                       for i, c in enumerate(color_clusters)]\n",
    "    fig.legend(handles=legend_elements, loc='center')\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sampling_uniform(data, sample_proportions):\n",
    "    assert sum(sample_proportions.values()) == 1, \\\n",
    "        sum(sample_proportions.values())\n",
    "\n",
    "    shuffled_data = data.sample(frac=1)\n",
    "    N = len(shuffled_data)\n",
    "    p_low = 0\n",
    "    index_to_name = {}\n",
    "    shuffled_data['Sample'] = -1\n",
    "    for i, (sample, proportion) in enumerate(sample_proportions.items()):\n",
    "        p_high = p_low + proportion\n",
    "        i_low, i_high = int(N * p_low), int(N * p_high)\n",
    "        shuffled_data.iloc[i_low: i_high, -1] = i\n",
    "        index_to_name[i] = sample\n",
    "        p_low = p_high\n",
    "\n",
    "    return shuffled_data, index_to_name\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distances"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dist_euclidean2(p1, p2):\n",
    "    d = p1.iloc[:-1] - p2.iloc[:-1]\n",
    "    return d.dot(d)\n",
    "\n",
    "\n",
    "def dist_manhattan(p1, p2):\n",
    "    return abs(p1.iloc[:-1] - p2.iloc[:-1]).sum()\n",
    "\n",
    "\n",
    "def dist_mahalanobis2(precision, p1, p2):\n",
    "    # precision matrix is the inverse of the covariance matrix\n",
    "    d = p1.iloc[:-1] - p2.iloc[:-1]\n",
    "    return d.transpose().dot(precision).dot(d)\n",
    "\n",
    "\n",
    "def closest(p, points, dist):\n",
    "    d = points.apply(lambda m: dist(p, m), axis=1)\n",
    "    return (d.argmin(), d.min())\n",
    "\n",
    "\n",
    "def normalize(points):\n",
    "    min_elem = points.iloc[:, :-1].min()\n",
    "    scale = points.iloc[:, :-1].max() - min_elem\n",
    "    def f(x): return ((x.iloc[:-1]-min_elem)/scale).append(x[-1:])\n",
    "    def g(x): return (x.iloc[:-1]*scale + min_elem).append(x[-1:])\n",
    "    return points.apply(f, axis=1), lambda x: x.apply(g, axis=1)\n",
    "\n",
    "\n",
    "def get_centroids(points):\n",
    "    points = points.groupby(points.columns[-1])\n",
    "    count = points.size()\n",
    "    means = points.mean().reset_index()\n",
    "    means.iloc[:,0] = count\n",
    "    cols = means.columns.tolist()\n",
    "    return means[cols[1:] + cols[:1]]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_internal_validation(points, centroids, dist):\n",
    "    #Calinski-Harabasz Criterion\n",
    "    m = points.mean()\n",
    "    m.iloc[-1] = -1\n",
    "    k = centroids.shape[0]\n",
    "    N = centroids.iloc[:, -1].sum()\n",
    "    SSW = sum(points.apply(lambda p: dist(\n",
    "        p, centroids.iloc[int(p.iloc[-1])]), axis=1))\n",
    "    SSB = sum(centroids.apply(lambda p: p.iloc[-1]*dist(p, m), axis=1))\n",
    "    VRC = (SSB/SSW) * ((N - k)/(k-1)) \n",
    "    return VRC\n",
    "\n",
    "\n",
    "def get_external_validation(points, answers):\n",
    "    def paired(t): return t.apply(lambda x: t.apply(lambda y: x == y))\n",
    "\n",
    "    def triang_inf(t): return t.mask(\n",
    "        np.triu(np.ones(t.shape)).astype(bool)).stack()\n",
    "\n",
    "    points = points.iloc[:, -1]\n",
    "    points = triang_inf(paired(points))\n",
    "    answers = triang_inf(paired(answers))\n",
    "    TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "    for u, v in zip(points, answers):\n",
    "        TP += u and v\n",
    "        FP += u and not v\n",
    "        FN += not u and v\n",
    "        TN += not u and not v\n",
    "\n",
    "    Pr = TP/(TP + FP)\n",
    "    R = TP/(TP + FN)\n",
    "    f1_measure = 2*(Pr * R)/(Pr + R)\n",
    "    return {'Precision': Pr, 'Recall': R, 'F1': f1_measure}\n",
    "\n",
    "\n",
    "def init_randompartition(points, k):\n",
    "    points.iloc[:, -1] = [random.randrange(0, k)\n",
    "                          for _ in range(points.shape[0])]\n",
    "    means = get_centroids(points)\n",
    "    return points, means\n",
    "\n",
    "\n",
    "def cluster_kmeans(points, dist, k):\n",
    "    points, means = init_randompartition(points, k)\n",
    "    objective = points.apply(lambda p: dist(\n",
    "        p, means.iloc[int(p[-1]), :]), axis=1).sum()\n",
    "    while True:\n",
    "        clusters_new, objective_new = assign_clusters(points, means, dist)\n",
    "        objective_new = sum(objective_new)\n",
    "        points.iloc[:, -1] = clusters_new\n",
    "        means_new = get_centroids(points)\n",
    "        delta = abs(objective - objective_new)\n",
    "        assert not np.isnan(delta)\n",
    "        if(delta < 0.0001):\n",
    "            return points, {i: i for i in range(k)}, means\n",
    "        means = means_new\n",
    "        objective = objective_new\n",
    "\n",
    "\n",
    "def assign_clusters(points, centroids, dist):\n",
    "    return zip(*points.apply(lambda p: closest(p, centroids, dist), axis=1))\n",
    "\n",
    "\n",
    "def train(points, method, dist):\n",
    "    best_1, best_2, best_3 = None, None, None\n",
    "    best_score = 0\n",
    "    for i in range(5):\n",
    "        trained, i_to_partition, centroids = method(points)\n",
    "        new_score = get_internal_validation(trained, centroids, dist)\n",
    "        if new_score > best_score:\n",
    "            best_1, best_2, best_3 = trained, i_to_partition, centroids\n",
    "            best_score = new_score\n",
    "    return best_1, best_2, best_3\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data, revert_norm = normalize(data)\n",
    "#plot_clusters(data, i_to_category)\n",
    "\n",
    "sample, i_to_partition = sampling_uniform(data, sample_proportions)\n",
    "#plot_clusters(sample.drop(sample.columns[-2], axis=1), i_to_partition)\n",
    "\n",
    "sample_train = sample.iloc[:, :-1].copy()\n",
    "trained, i_to_partition, centroids = train(\n",
    "    sample_train.copy(),\n",
    "    lambda points: cluster_kmeans(points, dist_euclidean2, 3),\n",
    "    dist_euclidean2)\n",
    "\n",
    "plot_clusters(revert_norm(trained), i_to_partition,\n",
    "              revert_norm(centroids))\n",
    "#plot_clusters(revert_norm(sample_train), i_to_category)\n",
    "\n",
    "print(get_external_validation(trained, sample_train.iloc[:, -1]).copy())\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "1a74cfba15cb2a7e1711077e0bf40c4aa90b3e0e84105adac214ba76b6a8694a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}