{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminars"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import random\n",
    "import colorsys\n",
    "from functools import partial\n",
    "from functools import reduce\n",
    "\n",
    "filename = 'Iris.csv'\n",
    "sample_proportions = {'Train': 0.6, 'Validation': 0.2, 'Test': 0.2}\n",
    "k = 3\n",
    "\n",
    "data = pd.read_csv(filename, index_col=0)\n",
    "name_to_index = {name: i for i, name in enumerate(data.iloc[:, -1].unique())}\n",
    "i_to_cluster = {i: name for name, i in name_to_index.items()}\n",
    "cluster = data.iloc[:, -1].apply(lambda x: name_to_index[x])\n",
    "data.drop(data.columns[-1], axis=1, inplace=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_clusters(data, cluster, i_to_name=None, centroid=None):\n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "    fig, axs = plt.subplots(data.shape[1], data.shape[1],\n",
    "                            sharex='col', sharey='row', constrained_layout=True)\n",
    "\n",
    "    if i_to_name is None:\n",
    "        N = cluster.unique().shape[0]\n",
    "        i_to_name = [i for i in range(N)]\n",
    "    else:\n",
    "        N = len(i_to_name)\n",
    "    def get_color(n): return colorsys.hsv_to_rgb(n/N, 0.65, 1)\n",
    "\n",
    "    color_data = cluster.apply(get_color)\n",
    "    color_cluster = [get_color(i) for i in range(N)]\n",
    "\n",
    "    for r, r_name in enumerate(data.columns):\n",
    "        y_data = data[r_name]\n",
    "        for c, c_name in enumerate(data.columns):\n",
    "            if r != c:\n",
    "                axs[r, c].scatter(data[c_name], y_data, c=color_data)\n",
    "            else:\n",
    "                axs[r, c].text(0.5, 0.5, r_name,\n",
    "                               fontweight='bold',\n",
    "                               horizontalalignment='center',\n",
    "                               verticalalignment='center',\n",
    "                               transform=axs[r, c].transAxes)\n",
    "\n",
    "    if centroid is not None:\n",
    "        for r, r_name in enumerate(data.columns):\n",
    "            y_centroid = centroid[r_name]\n",
    "            for c, c_name in enumerate(data.columns):\n",
    "                if r != c:\n",
    "                    axs[r, c].scatter(centroid[c_name], y_centroid, c=color_cluster,\n",
    "                                      marker=(5, 1), edgecolors='black')\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], label=i_to_name[i], markerfacecolor=c,\n",
    "                              color='w', marker='s', markersize=12)\n",
    "                       for i, c in enumerate(color_cluster)]\n",
    "    fig.legend(handles=legend_elements, loc='center')\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sampling_uniform(data, sample_proportions, cluster=None):\n",
    "    assert sum(sample_proportions.values()) == 1, \\\n",
    "        sum(sample_proportions.values())\n",
    "\n",
    "    N = data.shape[0]\n",
    "    idx = np.random.permutation(data.index)\n",
    "    data_r = data.reindex(idx)\n",
    "    cluster_r = None if cluster is None else cluster.reindex(idx)\n",
    "\n",
    "    p_low = 0\n",
    "    i_to_name = {}\n",
    "    data_r['Sample'] = None\n",
    "    for i, (sample, proportion) in enumerate(sample_proportions.items()):\n",
    "        p_high = p_low + proportion\n",
    "        i_low, i_high = int(N * p_low), int(N * p_high)\n",
    "        data_r.iloc[i_low: i_high, -1] = i\n",
    "        i_to_name[i] = sample\n",
    "        p_low = p_high\n",
    "\n",
    "    return data_r.drop('Sample', axis=1), data_r['Sample'], i_to_name, cluster_r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distances"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dist_euclidean2(p1, p2):\n",
    "    d = p1 - p2\n",
    "    return d.dot(d)\n",
    "\n",
    "\n",
    "def dist_manhattan(p1, p2):\n",
    "    return abs(p1 - p2).sum()\n",
    "\n",
    "\n",
    "def dist_mahalanobis2(precision, p1, p2):\n",
    "    # precision matrix is the inverse of the covariance matrix\n",
    "    d = p1 - p2\n",
    "    return d.transpose().dot(precision).dot(d)\n",
    "\n",
    "\n",
    "def closest(p, points, dist):\n",
    "    d = points.apply(lambda m: dist(p, m), axis=1)\n",
    "    return (d.argmin(), d.min())\n",
    "\n",
    "\n",
    "def normalize(points):\n",
    "    min_elem = points.min()\n",
    "    scale = points.max() - min_elem\n",
    "    def f(x): return (x-min_elem)/scale\n",
    "    def g(x): return x*scale + min_elem\n",
    "    return points.apply(f, axis=1), lambda x: x.apply(g, axis=1)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_internal_validation(points, cluster, centroids, dist):\n",
    "    # Calinski-Harabasz Criterion\n",
    "    m = points.mean()\n",
    "    k = centroids.shape[0]\n",
    "    N = points.shape[0]\n",
    "    n = centroids.apply(lambda c: (cluster == c.name).sum(), axis=1)\n",
    "    SSB = centroids.apply(lambda c: n[c.name]*dist(c, m), axis=1).sum()\n",
    "    SSW = points.apply(lambda p: dist(\n",
    "        p, centroids.loc[cluster[p.name]]), axis=1).sum()\n",
    "    VRC = (SSB/SSW) * ((N - k)/(k-1))\n",
    "    return VRC\n",
    "\n",
    "\n",
    "def get_external_validation(train_ans, real_ans):\n",
    "    def paired(t): return t.apply(lambda x: t.apply(lambda y: x == y))\n",
    "\n",
    "    def triang_inf(t): return t.mask(\n",
    "        np.triu(np.ones(t.shape)).astype(bool)).stack()\n",
    "\n",
    "    train_pair = triang_inf(paired(train_ans))\n",
    "    pair_ans = triang_inf(paired(real_ans))\n",
    "    TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "    for u, v in zip(train_pair, pair_ans):\n",
    "        TP += u and v\n",
    "        FP += u and not v\n",
    "        FN += not u and v\n",
    "        TN += not u and not v\n",
    "\n",
    "    Pr = TP/(TP + FP)\n",
    "    R = TP/(TP + FN)\n",
    "    f1_measure = 2*(Pr * R)/(Pr + R)\n",
    "    return {'Precision': Pr, 'Recall': R, 'F1': f1_measure}\n",
    "\n",
    "\n",
    "def cluster_kmeans(points, dist, k):\n",
    "    def get_centroids(points, membership): return pd.DataFrame(\n",
    "        [points[membership == i].mean() for i in range(k)])\n",
    "\n",
    "    def assign_cluster(points, centroid, dist):\n",
    "        c = points.apply(lambda p: pd.Series(\n",
    "            closest(p, centroid, dist)), axis=1)\n",
    "        return c[0], c[1].sum()\n",
    "\n",
    "    membership = points.apply(lambda _: random.randrange(0, k), axis=1)\n",
    "    centroid = get_centroids(points, membership)\n",
    "    objective = points.apply(lambda p: dist(\n",
    "        p, centroid.loc[membership[p.name]]), axis=1).sum()\n",
    "    delta = objective\n",
    "    while delta > 0.0001:\n",
    "        membership_new, objective_new = assign_cluster(points, centroid, dist)\n",
    "        centroid = get_centroids(points, membership_new)\n",
    "        delta = abs(objective - objective_new)\n",
    "        membership, objective = membership_new, objective_new\n",
    "    return membership, centroid\n",
    "\n",
    "\n",
    "def cluster_fuzzy_cmeans(points, dist, k, m):\n",
    "    print(points)\n",
    "    w = points.apply(lambda x: pd.Series(\n",
    "        [random.random() for _ in range(k)]), axis=1)\n",
    "    w = w.apply(lambda wi: wi/wi.sum(), axis=1)\n",
    "    centroids = pd.DataFrame([\n",
    "        points.iloc[:, :-1].mul(w[i], axis=0).sum()/w[i].sum() for i in range(k)])\n",
    "    centroids.iloc[:, -1] = 0\n",
    "    i, j = 0, 0\n",
    "    print(centroids.iloc[i, :].apply(lambda cj: 1/centroids.apply(\n",
    "        lambda ck: dist(cj, ck))))\n",
    "    print(centroids)\n",
    "    return True\n",
    "\n",
    "\n",
    "def train(points, method, dist):\n",
    "    best_ans, best_centroid = None, None\n",
    "    best_score = 0\n",
    "    for i in range(5):\n",
    "        ans, centroid = method(points, dist)\n",
    "        score = get_internal_validation(points, ans, centroid, dist)\n",
    "        if score > best_score:\n",
    "            best_score, best_ans, best_centroid = score, ans, centroid\n",
    "    return best_ans, best_centroid\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data, revert_norm = normalize(data)\n",
    "#plot_clusters(data, cluster, i_to_cluster)\n",
    "\n",
    "data_shuffle, data_sample, i_to_sample, cluster_shuffle = sampling_uniform(\n",
    "    data, sample_proportions, cluster)\n",
    "#plot_clusters(data_shuffle, data_sample, i_to_sample)\n",
    "\n",
    "data_train = data_shuffle[data_sample == 0].copy()\n",
    "cluster_train = cluster_shuffle[data_sample == 0].copy()\n",
    "\n",
    "ans_train, centroid_train = train(\n",
    "    data_train.copy(),\n",
    "    lambda p, dist: cluster_kmeans(p, dist, 3),\n",
    "    dist_euclidean2)\n",
    "plot_clusters(revert_norm(data_train), ans_train,\n",
    "              centroid = revert_norm(centroid_train))\n",
    "\n",
    "print(get_external_validation(ans_train, cluster_train))\n",
    "# cluster_fuzzy_cmeans(sample_train.iloc[:,:-2].copy(),\n",
    "#                     dist_euclidean2,\n",
    "#                     k, 2)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "1a74cfba15cb2a7e1711077e0bf40c4aa90b3e0e84105adac214ba76b6a8694a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}